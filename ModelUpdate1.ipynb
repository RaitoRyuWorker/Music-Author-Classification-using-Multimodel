{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8DECNPZAOMqU",
        "outputId": "c0296416-827d-444c-9d74-fef94a50490f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N-AqY4wEpSwo"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "import torchaudio\n",
        "import torch\n",
        "from torch import nn\n",
        "from torchsummary import summary\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vpgLg672wN96"
      },
      "outputs": [],
      "source": [
        "class MusicDataset(Dataset):\n",
        "\n",
        "    def __init__(self, \n",
        "                 annotations_file, \n",
        "                 audio_dir, \n",
        "                 transformation, \n",
        "                 target_sample_rate,\n",
        "                 num_samples,\n",
        "                 label_encoder,\n",
        "                 device):\n",
        "        self.annotations = pd.read_csv(annotations_file)\n",
        "        self.audio_dir = audio_dir\n",
        "        self.device = device\n",
        "        self.transformation = transformation.to(device)\n",
        "        self.target_sample_rate = target_sample_rate\n",
        "        self.num_samples = num_samples\n",
        "        self.label_encoder = label_encoder\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.annotations)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        audio_sample_path = self._get_audio_sample_path(index)\n",
        "        label = self._get_audio_sample_label(index)\n",
        "        signal, sr = torchaudio.load(audio_sample_path)\n",
        "        signal = signal.to(self.device)\n",
        "        signal = self._resample_if_necessary(signal, sr)\n",
        "        signal = self._mix_down_if_necessary(signal)\n",
        "        signal = self._cut_if_necessary(signal)\n",
        "        signal = self._right_pad_if_necessary(signal)\n",
        "        signal = self.transformation(signal)\n",
        "\n",
        "        label = np.array(label_encoder[label])\n",
        "        label = torch.from_numpy(label)\n",
        "        return signal, label\n",
        "\n",
        "    def _cut_if_necessary(self, signal):\n",
        "        if signal.shape[1] > self.num_samples:\n",
        "            signal = signal[:, :self.num_samples]\n",
        "        return signal\n",
        "\n",
        "    def _right_pad_if_necessary(self, signal):\n",
        "        length_signal = signal.shape[1]\n",
        "        if length_signal < self.num_samples:\n",
        "            num_missing_samples = self.num_samples - length_signal\n",
        "            last_dim_padding = (0, num_missing_samples)\n",
        "            signal = torch.nn.functional.pad(signal, last_dim_padding)\n",
        "        return signal\n",
        "\n",
        "    def _resample_if_necessary(self, signal, sr):\n",
        "        if sr != self.target_sample_rate:\n",
        "            resampler = torchaudio.transforms.Resample(sr, self.target_sample_rate)\n",
        "            resampler = resampler.to(self.device)\n",
        "            signal = resampler(signal)\n",
        "        return signal.to(self.device)\n",
        "\n",
        "    def _mix_down_if_necessary(self, signal):\n",
        "        if signal.shape[0] > 1:\n",
        "            signal = torch.mean(signal, dim=0, keepdim=True)\n",
        "        return signal\n",
        "\n",
        "    def _get_audio_sample_path(self, index):\n",
        "        return self.audio_dir + str(self.annotations.iloc[index, 3])\n",
        "\n",
        "    def _get_audio_sample_label(self, index):\n",
        "        return self.annotations.iloc[index, 0]\n",
        "\n",
        "    def _get_lyric(self, index):\n",
        "        return self.audio_dir + str(self.annotations.iloc[index, 2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-VtrEODzcVZ"
      },
      "outputs": [],
      "source": [
        "class CNNNetwork(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # 4 conv blocks / flatten / linear / softmax\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=1,\n",
        "                out_channels=16,\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                padding=2\n",
        "            ),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2)\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=16,\n",
        "                out_channels=32,\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                padding=2\n",
        "            ),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2)\n",
        "        )\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=32,\n",
        "                out_channels=64,\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                padding=2\n",
        "            ),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2)\n",
        "        )\n",
        "        self.conv4 = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=64,\n",
        "                out_channels=128,\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                padding=2\n",
        "            ),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2)\n",
        "        )\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear = nn.Linear(128 * 5 * 4, 10)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, input_data):\n",
        "        x = self.conv1(input_data)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.conv4(x)\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear(x)\n",
        "        predictions = self.softmax(logits)\n",
        "        return predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-8BZK_HxxjMV"
      },
      "outputs": [],
      "source": [
        "annotations_file = 'drive/MyDrive/khoa_luan/data_03/crawl_data.csv'\n",
        "audio_dir = 'drive/MyDrive/khoa_luan/data_03/'\n",
        "\n",
        "SAMPLE_RATE = 22050\n",
        "NUM_SAMPLES = 22050\n",
        "BATCH_SIZE = 128\n",
        "EPOCHS = 8\n",
        "LEARNING_RATE = 0.001\n",
        "\n",
        "mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n",
        "    sample_rate = SAMPLE_RATE,\n",
        "    n_fft = 1024,\n",
        "    hop_length = 512,\n",
        "    n_mels = 64)\n",
        "\n",
        "def create_data_loader(train_data, batch_size):\n",
        "    train_dataloader = DataLoader(train_data, batch_size=batch_size)\n",
        "    return train_dataloader\n",
        "\n",
        "\n",
        "def train_single_epoch(model, data_loader, loss_fn, optimiser, device):\n",
        "    for input, target in data_loader:\n",
        "        input, target = input.to(device), target.to(device)\n",
        "\n",
        "        # calculate loss\n",
        "        prediction = model(input)\n",
        "        loss = loss_fn(prediction, target)\n",
        "\n",
        "        # backpropagate error and update weights\n",
        "        optimiser.zero_grad()\n",
        "        loss.backward()\n",
        "        optimiser.step()\n",
        "\n",
        "    print(f\"loss: {loss.item()}\")\n",
        "\n",
        "\n",
        "def train(model, data_loader, loss_fn, optimiser, epochs, device):\n",
        "    for i in range(epochs):\n",
        "        print(f\"Epoch {i+1}\")\n",
        "        train_single_epoch(model, data_loader, loss_fn, optimiser, device)\n",
        "        print(\"---------------------------\")\n",
        "    print(\"Finished training\")\n",
        "\n",
        "label_encoder = {\n",
        "    'khắc hưng': 0,\n",
        "    'châu đăng khoa': 1,\n",
        "    'khắc việt': 2,\n",
        "    'phúc trường': 3,\n",
        "    'nguyễn đình vũ': 4,\n",
        "    'mr siro': 5,\n",
        "    'vương anh tú': 6,\n",
        "    'trịnh công sơn': 7,\n",
        "    'tiên cookie': 8,\n",
        "    'dương vỹ phúc': 9\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8qiwcol5lvs"
      },
      "source": [
        "#CNN\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8hHc8arhpBkc",
        "outputId": "b4d44e1c-8b1d-4742-ee91-fbab08712001"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device cuda\n",
            "CNNNetwork(\n",
            "  (conv1): Sequential(\n",
            "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (conv2): Sequential(\n",
            "    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (conv3): Sequential(\n",
            "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (conv4): Sequential(\n",
            "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear): Linear(in_features=2560, out_features=10, bias=True)\n",
            "  (softmax): Softmax(dim=1)\n",
            ")\n",
            "Epoch 1\n",
            "loss: 2.38285756111145\n",
            "---------------------------\n",
            "Epoch 2\n",
            "loss: 2.390868663787842\n",
            "---------------------------\n",
            "Epoch 3\n",
            "loss: 2.393054962158203\n",
            "---------------------------\n",
            "Epoch 4\n",
            "loss: 2.392770528793335\n",
            "---------------------------\n",
            "Epoch 5\n",
            "loss: 2.3924355506896973\n",
            "---------------------------\n",
            "Epoch 6\n",
            "loss: 2.390192747116089\n",
            "---------------------------\n",
            "Epoch 7\n",
            "loss: 2.3912668228149414\n",
            "---------------------------\n",
            "Epoch 8\n",
            "loss: 2.3925483226776123\n",
            "---------------------------\n",
            "Finished training\n",
            "Trained CNNNetwork saved at cnn.pth\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "        device = \"cuda\"\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "print(f\"Using device {device}\")\n",
        "\n",
        "Music = MusicDataset(annotations_file,\n",
        "                     audio_dir, \n",
        "                     mel_spectrogram, \n",
        "                     SAMPLE_RATE,\n",
        "                     NUM_SAMPLES,\n",
        "                     label_encoder,\n",
        "                     device)\n",
        "\n",
        "train_dataloader = create_data_loader(Music, BATCH_SIZE)\n",
        "cnn = CNNNetwork().to(device)\n",
        "print(cnn)\n",
        "\n",
        "# initialise loss funtion + optimiser\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimiser = torch.optim.Adam(cnn.parameters(),lr=LEARNING_RATE)\n",
        "\n",
        "# train model\n",
        "train(cnn, train_dataloader, loss_fn, optimiser, EPOCHS, device)\n",
        "\n",
        "# save model\n",
        "torch.save(cnn.state_dict(), \"cnn.pth\")\n",
        "print(\"Trained CNNNetwork saved at cnn.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hJ2z8tLiWUmP"
      },
      "outputs": [],
      "source": [
        "class_mapping = [\n",
        "    'khắc hưng',\n",
        "    'châu đăng khoa',\n",
        "    'khắc việt',\n",
        "    'phúc trường',\n",
        "    'nguyễn đình vũ',\n",
        "    'mr siro',\n",
        "    'vương anh tú',\n",
        "    'trịnh công sơn',\n",
        "    'tiên cookie',\n",
        "    'dương vỹ phúc'\n",
        "]\n",
        "\n",
        "def predict(model, input, target, class_mapping):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        predictions = model(input)\n",
        "        predicted_index = predictions[0].argmax(0)\n",
        "        predicted = class_mapping[predicted_index]\n",
        "        expected = class_mapping[target]\n",
        "    return predicted, expected"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Udi-EgVaWUoW"
      },
      "outputs": [],
      "source": [
        "# load back the model\n",
        "cnn = CNNNetwork().to(device)\n",
        "state_dict = torch.load(\"cnn.pth\")\n",
        "cnn.load_state_dict(state_dict)\n",
        "\n",
        "# load music dataset\n",
        "Music = MusicDataset(annotations_file,\n",
        "                     audio_dir, \n",
        "                     mel_spectrogram, \n",
        "                     SAMPLE_RATE,\n",
        "                     NUM_SAMPLES,\n",
        "                     label_encoder,\n",
        "                     device)\n",
        "\n",
        "true_rate = 0\n",
        "for i in range(len(Music)):\n",
        "  input, target = Music[i][0], Music[i][1] \n",
        "  input.unsqueeze_(0)\n",
        "  predicted, expected = predict(cnn, input, target,class_mapping)\n",
        "  if(predicted == expected):\n",
        "    true_rate += 1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"True predict rate: '{true_rate*100/len(Music)}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ttgh1M_2ME9",
        "outputId": "3543102a-4944-443b-f899-1bb22d6ff10e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True predict rate: '13.807531380753138'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1r_aazc5iCe"
      },
      "source": [
        "#SVM"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MusicDataset2(Dataset):\n",
        "\n",
        "    def __init__(self, \n",
        "                 annotations_file, \n",
        "                 audio_dir, \n",
        "                 transformation, \n",
        "                 target_sample_rate,\n",
        "                 num_samples,\n",
        "                 label_encoder,\n",
        "                 device):\n",
        "        self.annotations = pd.read_csv(annotations_file)\n",
        "        self.audio_dir = audio_dir\n",
        "        self.device = device\n",
        "        self.transformation = transformation.to(device)\n",
        "        self.target_sample_rate = target_sample_rate\n",
        "        self.num_samples = num_samples\n",
        "        self.label_encoder = label_encoder\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.annotations)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        audio_sample_path = self._get_audio_sample_path(index)\n",
        "        label = self._get_audio_sample_label(index)\n",
        "        signal, sr = torchaudio.load(audio_sample_path)\n",
        "        signal = signal.to(self.device)\n",
        "        signal = self._resample_if_necessary(signal, sr)\n",
        "        signal = self._mix_down_if_necessary(signal)\n",
        "        signal = self._cut_if_necessary(signal)\n",
        "        signal = self._right_pad_if_necessary(signal)\n",
        "        # signal = self.transformation(signal)\n",
        "\n",
        "        # label = np.array(label_encoder[label])\n",
        "        # label = torch.from_numpy(label)\n",
        "        return signal, label\n",
        "\n",
        "    def _cut_if_necessary(self, signal):\n",
        "        if signal.shape[1] > self.num_samples:\n",
        "            signal = signal[:, :self.num_samples]\n",
        "        return signal\n",
        "\n",
        "    def _right_pad_if_necessary(self, signal):\n",
        "        length_signal = signal.shape[1]\n",
        "        if length_signal < self.num_samples:\n",
        "            num_missing_samples = self.num_samples - length_signal\n",
        "            last_dim_padding = (0, num_missing_samples)\n",
        "            signal = torch.nn.functional.pad(signal, last_dim_padding)\n",
        "        return signal\n",
        "\n",
        "    def _resample_if_necessary(self, signal, sr):\n",
        "        if sr != self.target_sample_rate:\n",
        "            resampler = torchaudio.transforms.Resample(sr, self.target_sample_rate)\n",
        "            resampler = resampler.to(self.device)\n",
        "            signal = resampler(signal)\n",
        "        return signal.to(self.device)\n",
        "\n",
        "    def _mix_down_if_necessary(self, signal):\n",
        "        if signal.shape[0] > 1:\n",
        "            signal = torch.mean(signal, dim=0, keepdim=True)\n",
        "        return signal\n",
        "\n",
        "    def _get_audio_sample_path(self, index):\n",
        "        return self.audio_dir + str(self.annotations.iloc[index, 3])\n",
        "\n",
        "    def _get_audio_sample_label(self, index):\n",
        "        return self.annotations.iloc[index, 0]\n",
        "\n",
        "    def _get_lyric(self, index):\n",
        "        return self.audio_dir + str(self.annotations.iloc[index, 2])"
      ],
      "metadata": {
        "id": "dSh2NMokfVyV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "        device = \"cuda\"\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "print(f\"Using device {device}\")\n",
        "\n",
        "Music2 = MusicDataset2(annotations_file,\n",
        "                       audio_dir, \n",
        "                       mel_spectrogram, \n",
        "                       SAMPLE_RATE,\n",
        "                       NUM_SAMPLES,\n",
        "                       label_encoder,\n",
        "                       device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dd8MEEULgOfX",
        "outputId": "81a527b6-7646-48da-b654-e901b5ea8bd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "signal2 = []\n",
        "label2 = []\n",
        "for i in range(len(Music2)):\n",
        "    temp1, temp2 = Music2[i]\n",
        "    signal2.append(temp1.numpy())\n",
        "    label2.append(temp2)"
      ],
      "metadata": {
        "id": "aVBQpbXbgwZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eup5hTlorulq"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(signal2, label2, test_size = 0.2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hqGO1xVBruny"
      },
      "outputs": [],
      "source": [
        "svmclassifier = SVC(kernel = 'linear')\n",
        "svmclassifier.fit(X_train,y_train)\n",
        "y_pred = svmclassifier.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V_sNozy8rup8"
      },
      "outputs": [],
      "source": [
        "# check = []\n",
        "# for i in range(len(signal)):\n",
        "#   check.append(signal[i].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yxUZ8JxYruux"
      },
      "outputs": [],
      "source": [
        "# len(set(check))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xNg9tgv4KcCp"
      },
      "outputs": [],
      "source": [
        "# def unique(list1):\n",
        " \n",
        "#     # initialize a null list\n",
        "#     unique_list = []\n",
        " \n",
        "#     # traverse for all elements\n",
        "#     for x in list1:\n",
        "#         # check if exists in unique_list or not\n",
        "#         if x not in unique_list:\n",
        "#             unique_list.append(x)\n",
        "#     # print list\n",
        "#     for x in unique_list:\n",
        "#         print(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mXp9ryGSuqXa"
      },
      "outputs": [],
      "source": [
        "# temp = np.array(X_train)\n",
        "# unique(temp.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D0UZFBGXuqVW"
      },
      "outputs": [],
      "source": [
        "# type(y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "txVAA36KvJcg"
      },
      "outputs": [],
      "source": [
        "# signal[0].shape"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}